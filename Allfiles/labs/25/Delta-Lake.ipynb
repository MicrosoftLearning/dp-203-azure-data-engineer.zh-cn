{"cells":[{"cell_type":"markdown","source":["# Explore Delta Lake\n\nIn this notebook, you'll explore how to use Delta Lake in a Databricks Spark cluster.\n\n## Ingest data\n\nUse the **&#9656; Run Cell** menu option at the top-right of the following cell to run it and download a data file into the Databricks file system (DBFS)."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"bd65ba90-2574-461f-9083-0b9b9b6efd92","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["%sh\nrm -r /dbfs/data\nrm -r /dbfs/delta\nmkdir /dbfs/data\nwget -O /dbfs/data/products.csv https://raw.githubusercontent.com/MicrosoftLearning/dp-203-azure-data-engineer/master/Allfiles/labs/25/data/products.csv"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"5f4da022-9631-48a7-9835-2bb40f442c1d","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Now that you've ingested the data, you can load it into a Spark dataframe from the Databricks file system (DBFS)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"7ec1d7d0-ff8b-43eb-9a2a-e06d263e526e","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["df = spark.read.load('/data/products.csv', format='csv', header=True)\ndisplay(df.limit(10))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e17ea6fd-5c0c-4d5a-acc4-855ab5c9acb6","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Load the file data into a delta table\n\nYou can persist the data in the dataframe in Delta format using the following code:"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"47bcfa9b-355f-4147-854b-08597c467f3d","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["delta_table_path = \"/delta/products-delta\"\ndf.write.format(\"delta\").save(delta_table_path)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f8b91449-0aaf-4205-a07f-c7ef079c3dc3","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["The data for a delta lake table is stored in Parquet format. A log file is also created to track modifications made to the data.\n\nUse the following shell commands to view the contents of the folder where the delta data has been saved."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"6e9786b2-8f2c-4459-ad85-51bccaa8b486","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["%sh\nls /dbfs/delta/products-delta"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"66c68965-26b6-421d-8e40-af67ddf2cad7","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["The file data in Delta format can be loaded into a **DeltaTable** object, which you can use to view and upodate the data in the table. Run the following cell to update the data; reducing the price of product 771 by 10%."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"8031259b-45e8-4368-a2fc-4cbd6276c37d","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from delta.tables import *\nfrom pyspark.sql.functions import *\n\n# Create a deltaTable object\ndeltaTable = DeltaTable.forPath(spark, delta_table_path)\n\n# Update the table (reduce price of product 771 by 10%)\ndeltaTable.update(\n    condition = \"ProductID == 771\",\n    set = { \"ListPrice\": \"ListPrice * 0.9\" })\n\n# View the updated data as a dataframe\ndeltaTable.toDF().show(10)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"9c8c0b59-3426-4273-8ba6-a2cd3c1da897","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["The update is persisted to the data in the delta folder, and will be reflected in any new dataframe loaded from that location:"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a2b15ba3-9da3-40da-9124-9362e66f0e3a","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["new_df = spark.read.format(\"delta\").load(delta_table_path)\nnew_df.show(10)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"9ef3494d-b558-4292-be25-2f60cce6fdcf","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Data modifications are logged, enabling you to use the *time-travel* capabilities of Delta Lake to view previous versions of the data. For example, use the following code to view the original version of the product data:"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"aa87deb7-6a24-42ec-b116-606ddc850cec","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["new_df = spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(delta_table_path)\nnew_df.show(10)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"48215e8f-7435-40ec-9cf1-e710c5388f3a","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["The log contains a full history of modifications to the data. Use the following code to see a record of the last 10 changes:"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"baa34439-1934-4589-bbcd-3c10bfad691f","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["deltaTable.history(10).show(10, False, True)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e820df48-2483-454b-89ae-be51cdd40470","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Create catalog tables\n\nSo far you've worked with delta tables by loading data from the folder containing the parquet files on which the table is based. You can define *catalog tables* that encapsulate the data and provide a named table entity that you can reference in SQL code. Spark supports two kinds of catalog tables for delta lake:\n\n- *External* tables that are defined by the path to the parquet files containing the table data.\n- *Managed* tables, that are defined in the Hive metastore for the Spark cluster\n\n### Create an external table\n\nThe following code creates a new database named **AdventureWorks** and then creates an external tabled named **ProductsExternal** in that database based on the path to the Delta files you defined previously."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"3341a47a-f2a4-4dbf-be08-6753563d9dd0","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["spark.sql(\"CREATE DATABASE AdventureWorks\")\nspark.sql(\"CREATE TABLE AdventureWorks.ProductsExternal USING DELTA LOCATION '{0}'\".format(delta_table_path))\nspark.sql(\"DESCRIBE EXTENDED AdventureWorks.ProductsExternal\").show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"8d54592d-0818-44be-a2b6-3a3a70e06e14","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Note that the **Location** property of the new table is the path you specified.\n\nYou can query the new table by using a SQL `SELECT` statement, like this:"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"06df3d0a-94d6-43a7-bc8e-bdd243fd3c29","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["%sql\nUSE AdventureWorks;\n\nSELECT * FROM ProductsExternal;"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"bf082860-c29c-4a1b-9a29-9894661b6cdc","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Create a managed table\n\nA *managed* table stores its data files in the Hive metastore for the Spark cluster.\n\nRun the following code to create (and then describe) a managed tabled named **ProductsManaged** based on the dataframe you originally loaded from the **products.csv** file (before you updated the price of product 771)."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d25721a1-8ba5-4487-8aab-308423a05fc9","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["df.write.format(\"delta\").saveAsTable(\"AdventureWorks.ProductsManaged\")\nspark.sql(\"DESCRIBE EXTENDED AdventureWorks.ProductsManaged\").show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"04b73c5e-4309-41b6-b20c-d9127668790c","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["You did not specify a path for the parquet files used by the table - this is managed for you in the Hive metastore, and shown in the **Location** property in the table description (in the **dbfs:/user/hive/warehouse/** path).\n\nFrom the SQL user's perspective, there's no difference between external and managed tables when it comes to querying them with a `SELECT` statement:"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f554a23b-d628-49ce-b01d-22c7fd771b77","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["%sql\nUSE AdventureWorks;\n\nSELECT * FROM ProductsManaged;"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"341b2f1a-7f15-4391-b71b-5e2846b8d73f","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Compare external and managed tables\n\nlet's explore the differences between external and managed tables.\n\nFirst, use the following code to list the tables in the **AdventureWorks** database:"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"71079a2e-20db-4c96-9d3a-61624412919e","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["%sql\nUSE AdventureWorks;\n\nSHOW TABLES;"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"3d4eaf84-b645-4502-8166-b40bfcf88386","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Now let's take a look at the folders on which these tables are based:"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e3b82f23-d031-4979-8842-77b0b3d2d9b5","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["%sh\necho \"External table:\"\nls /dbfs/delta/products-delta\necho\necho \"Managed table:\"\nls /dbfs/user/hive/warehouse/adventureworks.db/productsmanaged"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"8df8da51-4949-45b7-aa39-8d44c92c2db7","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["What happens if we use a `DROP` statement to delete these tables from the database?"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"69e77412-88ec-42d7-8207-e8d419e98b39","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["%sql\n\nUSE AdventureWorks;\n\nDROP TABLE IF EXISTS ProductsExternal;\nDROP TABLE IF EXISTS ProductsManaged;\n\nSHOW TABLES;"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b4fe298f-b1e1-4a07-ae99-0e6b684356f6","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["The metadata for both tables has beenr emoved from the database; but what about the delta files?"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"65c4770f-a6d6-4bd2-81b7-4874d2306596","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["%sh\necho \"External table:\"\nls /dbfs/delta/products-delta\necho\necho \"Managed table:\"\nls /dbfs/user/hive/warehouse/adventureworks.db/productsmanaged"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c9ced79f-f7d8-452d-8e05-497af3efaef0","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["The files for the managed table are deleted automatically when the table is dropped. However, the files for the external table remain. Dropping an external table only removes the table metadata from the database; it does not delete the data files.\n\nYou can use the dfollowing code to create a new table in the database that is based on the delta files in the **products-delta** folder:"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a65f3f04-87fe-4c26-8854-f78d98cd21c2","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["%sql\nUSE AdventureWorks;\n\nCREATE TABLE Products\nUSING DELTA\nLOCATION '/delta/products-delta';"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"0ae5680a-d328-4a05-b68c-f733a4450d51","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Now you can query the new table"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"157adb38-09a2-4b84-ac2d-be11005b04b7","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["%sql\nUSE AdventureWorks;\n\nSELECT * FROM Products;"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e23883b2-a80c-40c6-81d9-a0d8381fc5a9","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Because the table is based on the existing delta files, which include the logged history of changes, it reflects the modifications you previously made to the products data.\n\n## Use delta tables for streaming data\n\nDelta lake supports streaming data. Delta tables can be a *sink* or a *source* for data streams created using the Spark Structured Streaming API. In this example, you'll use a delta table as a sink for some streaming data in a simulated internet of things (IoT) scenario.\n\nFirst, let's get some simulated device data in JSON format. Run the following cell to download a JSON file that looks like this:\n\n```json\n{\"device\":\"Dev1\",\"status\":\"ok\"}\n{\"device\":\"Dev1\",\"status\":\"ok\"}\n{\"device\":\"Dev1\",\"status\":\"ok\"}\n{\"device\":\"Dev2\",\"status\":\"error\"}\n{\"device\":\"Dev1\",\"status\":\"ok\"}\n{\"device\":\"Dev1\",\"status\":\"error\"}\n{\"device\":\"Dev2\",\"status\":\"ok\"}\n{\"device\":\"Dev2\",\"status\":\"error\"}\n{\"device\":\"Dev1\",\"status\":\"ok\"}\n```"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"3d2b9bb7-2fe2-43d5-ba71-339f3aab68a1","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["%sh\nrm -r /dbfs/device_stream\nmkdir /dbfs/device_stream\nwget -O /dbfs/device_stream/devices1.json https://raw.githubusercontent.com/MicrosoftLearning/dp-203-azure-data-engineer/master/Allfiles/labs/25/data/devices1.json\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e5f1b5e4-5754-498e-b36f-09f9d7e310ae","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Now you're ready to use Spark Structured Steraming to create a stream based on the folder containing the JSON device data."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"03ed4dce-1138-4040-80c2-366e53500e6b","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from pyspark.sql.types import *\nfrom pyspark.sql.functions import *\n\n# Create a stream that reads data from the folder, using a JSON schema\ninputPath = '/device_stream/'\njsonSchema = StructType([\nStructField(\"device\", StringType(), False),\nStructField(\"status\", StringType(), False)\n])\niotstream = spark.readStream.schema(jsonSchema).option(\"maxFilesPerTrigger\", 1).json(inputPath)\nprint(\"Source stream created...\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"cbe13007-95c6-454f-8762-b17e67d80abd","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Now you'll take the stream of data you're reading from the folder, and perpetually write it to a delta table folder:"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"5195a7ad-d632-4b0d-b2e0-ada404b950d7","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# Write the stream to a delta table\ndelta_stream_table_path = '/delta/iotdevicedata'\ncheckpointpath = '/delta/checkpoint'\ndeltastream = iotstream.writeStream.format(\"delta\").option(\"checkpointLocation\", checkpointpath).start(delta_stream_table_path)\nprint(\"Streaming to delta sink...\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"7113df46-d34b-4975-881f-dd1af62b2b88","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["To load the streamed table data, just read the delta table folder source like any other dataframe:"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"1c8ed435-ad19-4517-a93c-d0d95f32abfc","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# Read the data in delta format into a dataframe\ndf = spark.read.format(\"delta\").load(delta_stream_table_path)\ndisplay(df)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"4d3e5d71-1f57-4c7d-a6ea-586c3386ee48","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["You can also create a table based on the streaming delta table folder:"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"827a4c99-da37-43e3-bb8d-f45ea2b201a7","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# create a catalog table based on the streaming sink\nspark.sql(\"CREATE TABLE IotDeviceData USING DELTA LOCATION '{0}'\".format(delta_stream_table_path))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"6409e50a-3950-4ef9-ab06-88e40d79b2fb","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["You can query the table just like any other:"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"5db839b8-481f-455c-8dc7-d42bcad7b616","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["%sql\nSELECT * FROM IotDeviceData;"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"368f1697-7877-486c-9aa4-f5c675928121","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Now let's add some fresh device data to the stream."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ad282c03-0bcf-4832-acf4-9c023d0e22cd","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["%sh\nwget -O /dbfs/device_stream/devices2.json https://raw.githubusercontent.com/MicrosoftLearning/dp-203-azure-data-engineer/master/Allfiles/labs/25/data/devices2.json"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c5439c7b-cbb3-4695-9d9b-9f74bd4fb3aa","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["The new JSON data in the device folder is read into the stream and written to the delta folder, where it is reflected in the table:"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"35410a64-0fd1-4581-8fb6-9900534001da","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["%sql\nSELECT * FROM IotDeviceData;"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a2e52f25-0bf5-4ada-ae30-52a9445a7c1c","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["To stop the stream, use its **stop** method:"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ebbff25f-f1b4-4a77-bac1-c17201ce4a3b","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["deltastream.stop()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"0daabdea-3cb7-4768-bb6b-1f4051a61284","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Explore Delta Lake","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4,"mostRecentlyExecutedCommandWithImplicitDF":{"commandId":900962100515799,"dataframes":["_sqldf"]}},"language":"python","widgets":{},"notebookOrigID":4121592056562953}},"nbformat":4,"nbformat_minor":0}
